{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>TP1</h1>\n",
    "<h3>Júlia Togashi de Miranda</h3>\n",
    "<h3>Pedro Germano Almeida Machado</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "[0.2708333333333333, 0.13541666666666666, 0.15625, 0.14583333333333331, 0.13541666666666666, 0.15625]\n",
      "4\n",
      "[0.2573333333333333, 0.13626666666666665, 0.15599999999999997, 0.1581333333333333, 0.13626666666666665, 0.15599999999999997]\n"
     ]
    }
   ],
   "source": [
    "#EXERCISE 1\n",
    "def PageRank (filename, beta, epsilon):\n",
    "    M, num_nodes= Make_Matrix_M(filename)#We make matrix M\n",
    "    pi =[]\n",
    "    P_pi=[]\n",
    "    count=0\n",
    "    for i in range(num_nodes):\n",
    "        pi.append(1/num_nodes) #Define starting vector of importances\n",
    "    add_vec=[]\n",
    "    for i in range(num_nodes):\n",
    "        add_vec.append((1.0-beta)/num_nodes) #Define vector of prob to random jumps\n",
    "    loop=True\n",
    "    while loop:\n",
    "        P_pi=pi\n",
    "        count+=1\n",
    "        mult= Mult_Matrix(M,P_pi,num_nodes)\n",
    "        pi= [sum(x) for x in zip((y*beta for y in mult), add_vec)] #multiply M and pi and add vector of prob to ramdom jumps\n",
    "        dif=0.0\n",
    "        for i in range(len(pi)-1):\n",
    "            dif= dif+abs(pi[i]-P_pi[i]) #calculate the norm of the difference in the last interatiom\n",
    "        if dif <epsilon:\n",
    "            loop= False #if difference is less than epsilon, will stop\n",
    "    print(count)\n",
    "    return pi # return probabilities\n",
    "              \n",
    "\n",
    "def Make_Matrix_M(filename): #Define construct of Matrix M given the file that contains the edges of our graph\n",
    "    G=open(filename).read().splitlines() #open file and read it line by line: result is list with elements \"i j\"\n",
    "    #print(G)\n",
    "    M=[] #define sparce matrix M, which will be a list of lines \"line col value\"\n",
    "    out_degree=[]#define vector to compute the out-degrees for each node\n",
    "    for edge in G:\n",
    "        E=edge.split()\n",
    "        M.append(E[1]+\" \"+E[0]) # in PageRank we have Mij equals to 1/ out_degree(Vj) if there is a edge vj to vi, and 0 if not.\n",
    "        #So we will have that the node that the edge is exiting will be the col, and the node to were it is arriving will be the line.\n",
    "        if int(E[0]) <= len(out_degree): #for the node which the edge is going out\n",
    "            out_degree[int(E[0])-1]+=1 #if the out-degree is already computed, we will sum 1\n",
    "        else:#if not\n",
    "            for i in range (int(E[0])-len(out_degree)-1):\n",
    "                out_degree.append(0) \n",
    "                #this is in case the file is not ordered, so we will add zero to to the indexes that are still missing\n",
    "            out_degree.append(1)\n",
    "        #print(E[0],out_degree)\n",
    "    num_nodes=len(out_degree) #saving num of nodes to help later\n",
    "    for m in range(0,len(M)): #we will go throuht all the elements in M to add the value according to the out-degree of the cols\n",
    "        entry=M[m].split()\n",
    "        M[m] = M[m] +\" \"+ str(1/out_degree[int(entry[1])-1]) #here we add the values to the matrix M\n",
    "    M.sort()\n",
    "    #print(M)\n",
    "    return M, num_nodes\n",
    "\n",
    "\n",
    "def Mult_Matrix(M,pi,num_nodes): #func to multiply sparce matrix M for the vector pi\n",
    "    product=[]\n",
    "    for i in range(num_nodes):\n",
    "        product.append(0.0)#initialize the product result vector\n",
    "        \n",
    "    for m in range(0,len(M)):#for each entry in the Matrix M\n",
    "        entry=M[m].split()\n",
    "        #we update the product: equals the old one, plus the value of Mij times Pj\n",
    "        product[int(entry[0])-1]=product[int(entry[0])-1]+(float(entry[2])*pi[int(entry[1])-1])\n",
    "    #print(product,\"\\n\")\n",
    "    return product\n",
    "\n",
    "#Results \n",
    "print(PageRank(\"Figure 1.txt\",1.0,0.1))\n",
    "print(PageRank(\"Figure 1.txt\",0.8,0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Question 1</h2>\n",
    "\n",
    "<blockquote>\n",
    "We have that the parameter beta corresponds to the improvement in the Random Surfer 2.0, were our random surfer have a 1-beta probability of jumping randomly to any page, that might have or not a link with our current page. This probability being different to one not only resolves our problem with spider traps and dead-ends (we have a probability of jumping randomly to some node out of the spider trap), but also it gives us a guarantee that our algorithm will converge to the same result, independent of the node that we started. Beta would in theory make our sparce matrix M became a dense matrix A (few entries different to zero). This do doesn’t represent a problem, as we can represent M plus a vector.\n",
    "\n",
    "Epsilon on the other hand is present in the two implementations of the random surfer, and is the error that we consider to say that our distance vector converged.\n",
    "\n",
    "In our example we found really similar results for the beta=1.0, which means no random jumps, and with beta=0.8, that means that we have an 0.2 probability of doing a random jump. Even though we got similar results, without random jumps we don’t necessarily have an irreducible/aperiodic MC, so we cannot affirm that it will converge to a unique stationary state.\n",
    "\n",
    "One last interesting thing is that with beta=0.8 we converged to the result faster, which also makes sense, as you can go through the hole graph in less interations.\n",
    "</blockquote>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04131802558877686, 0.0028437666903257737, 0.008930447092086694, 0.014159950389678813, 0.030702869024088156, 0.024042736568291673, 0.015670742052588782, 0.03334802002121073, 0.021701878957824824, 0.021768158286536676, 0.011592120068955573, 0.018155316504560676, 0.025032384249882814, 0.009045331622254455, 0.041639582808086134, 0.010903922374964525, 0.005950215074169164, 0.017603463231141387, 0.013138302846448732, 0.011566889841573677, 0.025291344373788143, 0.01576338943426806, 0.02467696998339421, 0.023548725569537996, 0.0184278304200199, 0.008556235257364816, 0.025747333710766117, 0.03461300886742758, 0.00499584578226003, 0.0017286698264504918, 0.012778648875933897, 0.02883400152842605, 0.015172250679808931, 0.012992516211991583, 0.014992044729718825, 0.007144677900102733, 0.0055127444986664605, 0.031134835403566635, 0.01512462866112654, 0.037629258318690455, 0.014538968794507591, 0.009286305535354191, 0.051298872830864824, 0.0015660573761481797, 0.02810351340163479, 0.01714646514295769, 0.001286506289051587, 0.013597742085541065, 0.013509056155386275, 0.020413682147951393, 0.0045601058711433225, 0.016679919076478516, 0.019990977697940726, 0.019039098391518128, 0.025203645876765993] \n",
      "\n",
      "[0.036043163935188, 0.006437825294505133, 0.011021148159934012, 0.014609548691702053, 0.028285586662276294, 0.02229786488044728, 0.015252586988589462, 0.030998397419225814, 0.02157308685464078, 0.02142338735031625, 0.013692872308238032, 0.019995795164381498, 0.02290946417532976, 0.011584430168966793, 0.03769798089322775, 0.012247767367414482, 0.009940922477667478, 0.01953477007592687, 0.013274138791935468, 0.013053598733385522, 0.022864381113739497, 0.016305638133939086, 0.02191004575178612, 0.022020993753546762, 0.018901137587770157, 0.010902499661082497, 0.023402968504559658, 0.03177906580461608, 0.0075918263213862565, 0.004999033189466054, 0.013747960313756233, 0.026578277077761753, 0.015054672637344464, 0.013256455666009461, 0.01573986211119905, 0.00986454146395562, 0.008324826513882225, 0.027260647672818935, 0.0163859845884481, 0.033846404231555396, 0.015656514715320802, 0.010616903570837968, 0.04441976195553261, 0.004816090634229508, 0.02534583070120758, 0.01761615281190912, 0.004391434779142758, 0.014799643868674012, 0.01366604638058971, 0.020476820909477538, 0.0071282687847193955, 0.017103092899911215, 0.019935707564494268, 0.01861057560400385, 0.022805596328027474]\n"
     ]
    }
   ],
   "source": [
    "#EXERCISE 2\n",
    "import re,os\n",
    "\n",
    "files = os.listdir(\"WebPages\") #here we are getting reference to all webpages\n",
    "conversion = {} #as in our PageRank algorithm we considered the nodes of the graph numbers starting from one, we will need to convert the webpages names to numbers consistently\n",
    "#We decided to use a dicionary because the search in the dictionary is more efficient than findind a index of a list\n",
    "for i in range(0,len(files)):\n",
    "    conversion[files[i]] = i + 1 #our entries in the diccionary\n",
    "#print(conversion.get(\"Assembly_language.html\"))\n",
    "\n",
    "script_dir = os.path.dirname(os.path.abspath(\"TP1.ipynb\")) #path\n",
    "\n",
    "Graph = open(\"webpage_graph.txt\",\"w+\")\n",
    "for webpage in files: #for each webpage we will find the liks in its body\n",
    "    i = conversion.get(webpage)\n",
    "    #print(i,webpage)\n",
    "    rel_path = \"WebPages\"\n",
    "    abs_file_path = os.path.join(script_dir, rel_path,webpage)\n",
    "    page = open(abs_file_path,encoding = \"utf8\").read() #path\n",
    "    #print(page)\n",
    "    links = re.findall('a href=\"\\S+\"',page) #find links using the fact that they all start with a href=\n",
    "    #print(links)\n",
    "    for l in range(0,len(links)): #lets remove the a href= and quotation marks to make them consistent with our diccionary enties\n",
    "        links[l] = links[l].replace('a href=', '')\n",
    "        links[l] = links[l].replace('\"', '')\n",
    "        \n",
    "    links = list(dict.fromkeys(links)) #remove duplicates\n",
    "    if webpage in links:\n",
    "        links.remove(webpage) #remove selfloop\n",
    "    #print(links, \"\\n\",\"\\n\")\n",
    "    \n",
    "    for l in links: \n",
    "        j = conversion.get(l)\n",
    "        Graph.write(str(i) + \" \" + str(j)+\"\\n\") #writes edges in file\n",
    "        \n",
    "Graph.close()\n",
    "\n",
    "#Results\n",
    "print(PageRank(\"webpage_graph.txt\",1.0,0.1),\"\\n\")\n",
    "print(PageRank(\"webpage_graph.txt\",0.8,0.1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assembly_language.html \n",
      " 0.04131802558877686 \n",
      " \n",
      "\n",
      "Binary_file.html \n",
      " 0.0028437666903257737 \n",
      " \n",
      "\n",
      "Boolean_data_type.html \n",
      " 0.008930447092086694 \n",
      " \n",
      "\n",
      "Bytecode.html \n",
      " 0.014159950389678813 \n",
      " \n",
      "\n",
      "C++.html \n",
      " 0.030702869024088156 \n",
      " \n",
      "\n",
      "COBOL.html \n",
      " 0.024042736568291673 \n",
      " \n",
      "\n",
      "Comparison_of_programming_languages.html \n",
      " 0.015670742052588782 \n",
      " \n",
      "\n",
      "Compiler.html \n",
      " 0.03334802002121073 \n",
      " \n",
      "\n",
      "Computer.html \n",
      " 0.021701878957824824 \n",
      " \n",
      "\n",
      "Computer_hardware.html \n",
      " 0.021768158286536676 \n",
      " \n",
      "\n",
      "Computer_memory.html \n",
      " 0.011592120068955573 \n",
      " \n",
      "\n",
      "Computer_program.html \n",
      " 0.018155316504560676 \n",
      " \n",
      "\n",
      "Computer_science.html \n",
      " 0.025032384249882814 \n",
      " \n",
      "\n",
      "Control_flow.html \n",
      " 0.009045331622254455 \n",
      " \n",
      "\n",
      "C_(programming_language).html \n",
      " 0.041639582808086134 \n",
      " \n",
      "\n",
      "Database.html \n",
      " 0.010903922374964525 \n",
      " \n",
      "\n",
      "Data_(computing).html \n",
      " 0.005950215074169164 \n",
      " \n",
      "\n",
      "Data_type.html \n",
      " 0.017603463231141387 \n",
      " \n",
      "\n",
      "Dynamic_programming_language.html \n",
      " 0.013138302846448732 \n",
      " \n",
      "\n",
      "Executable.html \n",
      " 0.011566889841573677 \n",
      " \n",
      "\n",
      "Fortran.html \n",
      " 0.025291344373788143 \n",
      " \n",
      "\n",
      "GNU_Compiler_Collection.html \n",
      " 0.01576338943426806 \n",
      " \n",
      "\n",
      "High-level_programming_language.html \n",
      " 0.02467696998339421 \n",
      " \n",
      "\n",
      "Imperative_programming.html \n",
      " 0.023548725569537996 \n",
      " \n",
      "\n",
      "Instruction_set.html \n",
      " 0.0184278304200199 \n",
      " \n",
      "\n",
      "Integer_(computer_science).html \n",
      " 0.008556235257364816 \n",
      " \n",
      "\n",
      "JavaScript.html \n",
      " 0.025747333710766117 \n",
      " \n",
      "\n",
      "Java_(programming_language).html \n",
      " 0.03461300886742758 \n",
      " \n",
      "\n",
      "Kernel_(computing).html \n",
      " 0.00499584578226003 \n",
      " \n",
      "\n",
      "Lexical_scope.html \n",
      " 0.0017286698264504918 \n",
      " \n",
      "\n",
      "Linux.html \n",
      " 0.012778648875933897 \n",
      " \n",
      "\n",
      "Lisp_(programming_language).html \n",
      " 0.02883400152842605 \n",
      " \n",
      "\n",
      "List_of_programming_languages.html \n",
      " 0.015172250679808931 \n",
      " \n",
      "\n",
      "Logic_programming.html \n",
      " 0.012992516211991583 \n",
      " \n",
      "\n",
      "Low-level_programming_language.html \n",
      " 0.014992044729718825 \n",
      " \n",
      "\n",
      "Memory_address.html \n",
      " 0.007144677900102733 \n",
      " \n",
      "\n",
      "Method_(computer_programming).html \n",
      " 0.0055127444986664605 \n",
      " \n",
      "\n",
      "Object-oriented_programming.html \n",
      " 0.031134835403566635 \n",
      " \n",
      "\n",
      "Object_(computer_science).html \n",
      " 0.01512462866112654 \n",
      " \n",
      "\n",
      "Operating_system.html \n",
      " 0.037629258318690455 \n",
      " \n",
      "\n",
      "Pointer_(computer_programming).html \n",
      " 0.014538968794507591 \n",
      " \n",
      "\n",
      "Porting.html \n",
      " 0.009286305535354191 \n",
      " \n",
      "\n",
      "Programming_language.html \n",
      " 0.051298872830864824 \n",
      " \n",
      "\n",
      "Program_(machine).html \n",
      " 0.0015660573761481797 \n",
      " \n",
      "\n",
      "Python_(programming_language).html \n",
      " 0.02810351340163479 \n",
      " \n",
      "\n",
      "Scripting_language.html \n",
      " 0.01714646514295769 \n",
      " \n",
      "\n",
      "Snowball_programming_language.html \n",
      " 0.001286506289051587 \n",
      " \n",
      "\n",
      "Software.html \n",
      " 0.013597742085541065 \n",
      " \n",
      "\n",
      "Software_portability.html \n",
      " 0.013509056155386275 \n",
      " \n",
      "\n",
      "Source_code.html \n",
      " 0.020413682147951393 \n",
      " \n",
      "\n",
      "Strong_and_weak_typing.html \n",
      " 0.0045601058711433225 \n",
      " \n",
      "\n",
      "Subroutine.html \n",
      " 0.016679919076478516 \n",
      " \n",
      "\n",
      "Type_system.html \n",
      " 0.019990977697940726 \n",
      " \n",
      "\n",
      "Unix.html \n",
      " 0.019039098391518128 \n",
      " \n",
      "\n",
      "Virtual_machine.html \n",
      " 0.025203645876765993 \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "x=PageRank(\"webpage_graph.txt\",1.0,0.1)\n",
    "for i in range (0,len(x)):\n",
    "    print(files[i],\"\\n\",x[i],\"\\n\",\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Question 2</h2>\n",
    "\n",
    "<blockquote>\n",
    "We decided to deal with the duplicate links and self-loops removing them. As was commented in class, one way to take advantage of the page rank algorithm to boost your page would be to make a lot of self-loops, or creating a small amount of web pages and making a lot of connections between them. As was also said, the modern page rank algorithm as used in google has mechanisms to blook that. So, for that reason we just decided to remove them.<br>\n",
    "    \n",
    "    \n",
    "As we can see in the results, now that we have more pages, and a more complexed graph, we can see a bigger different in results from when we use beta equal to one or beta equal to 0.8. One other interesting thing in the results is that the importance of pages is somehow balanced. There are of course differences between importance’s, (there are some pages with clearly less importance, like Integer_(computer_science).html with approximately 0.8%.\n",
    "\n",
    "One last thing worth to notice is that indeed, if we try to interpreter the results, we will see that they seem to make sense. Web pages like Java or Python have a considerable larger importance, and they are probably pages more searched by a larger public than something really specific, like kernel, which has an smaller importance.\n",
    "\n",
    "</blockquote>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Graph before adding dead-ends: \n",
      " ['1 2', '2 3', '3 4', '4 1', '1 5', '5 6', '6 1'] \n",
      "\n",
      "\n",
      "['1 2', '2 3', '3 4', '4 1', '1 5', '5 6', '6 1', '1 7', '7 8', '7 9', '8 9'] \n",
      " [3, 1, 1, 1, 1, 1, 2, 1, 0] \n",
      "\n",
      "['1 2', '2 3', '3 4', '4 1', '1 5', '5 6', '6 1', '1 7', '7 8'] \n",
      " [3, 1, 1, 1, 1, 1, 1, 0, -1] \n",
      "\n",
      "['1 2', '2 3', '3 4', '4 1', '1 5', '5 6', '6 1', '1 7'] \n",
      " [3, 1, 1, 1, 1, 1, 0, -1, -1] \n",
      "\n",
      "['1 2', '2 3', '3 4', '4 1', '1 5', '5 6', '6 1']\n"
     ]
    }
   ],
   "source": [
    "#EXERCISE 3\n",
    "def Remove_dead_ends(filename): #Define func to remove dead-ends from the graph\n",
    "    G=open(filename).read().splitlines()#open file and read it line by line: result is list with elements \"i j\"\n",
    "    out_degree=[]#define vector to compute the out-degrees for each node\n",
    "    \n",
    "    print(\"Test: Graph before adding dead-ends: \\n\",G,\"\\n\\n\")\n",
    "    #adding dead-ends to test\n",
    "    G.append(\"1 7\")\n",
    "    G.append(\"7 8\")\n",
    "    G.append(\"7 9\")\n",
    "    G.append(\"8 9\")\n",
    "    \n",
    "    for edge in G:\n",
    "        E=edge.split()\n",
    "        if int(E[0]) <= len(out_degree): #for the node which the edge is going out\n",
    "            out_degree[int(E[0])-1]+=1 #if the out-degree is already computed, we will sum 1\n",
    "        else:#if not\n",
    "            for i in range (int(E[0])-len(out_degree)-1):\n",
    "                out_degree.append(0) \n",
    "                #this is in case the file is not ordered, so we will add zero to to the indexes that are still missing\n",
    "            out_degree.append(1)\n",
    "            \n",
    "        if int(E[1]) > len(out_degree): #As we have dead-ends now, we have to make sure all our nodes are in the out_degree vector\n",
    "            #So now we also add the entry equals zero to the j node in the edge, if it's still not there\n",
    "            for i in range (int(E[1])-len(out_degree)):\n",
    "                out_degree.append(0)\n",
    "                \n",
    "        #print(E[0],out_degree)       \n",
    "             \n",
    "    while 0 in out_degree: #until there are no more dead-ends\n",
    "        removes=[] #list of nodes that need to be removed from the graph in each interection\n",
    "        new_G=[] #create G without the dead-ends removed in that interaction\n",
    "        print(G,\"\\n\",out_degree,\"\\n\")\n",
    "        for i in range(0,len(out_degree)):\n",
    "            if out_degree[i]==0: #if out-links numbers equals to zero\n",
    "                removes.append(i+1) #add node to list of nodes to be removed\n",
    "                out_degree[i]=-1#we set the value to -1, so that we know that node is no loger there, and we don't have problems with indexing \n",
    "    \n",
    "        for edge in G: \n",
    "            E=edge.split()\n",
    "            if int(E[1]) in removes: #if the node is a dead-end\n",
    "                out_degree[int(E[0])-1]-=1  # we decresce the out-degree of the nodes with an edge to it\n",
    "            else:\n",
    "                new_G.append(edge) #append edges that did't lead to a dead-end in that interection\n",
    "        G=new_G\n",
    "    \n",
    "    Graph = open(\"graph_no_dead_ends.txt\",\"w+\")\n",
    "    for edge in G:\n",
    "        Graph.write(edge+\"\\n\") #writes edges in new file without dead-ends\n",
    "        \n",
    "    Graph.close()\n",
    "    \n",
    "    return G\n",
    "\n",
    "print(Remove_dead_ends(\"Figure 1.txt\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Question 3</h2>\n",
    "\n",
    "<blockquote>\n",
    "    \n",
    "As we saw in class, we can define spider traps as a set of pages S not containing any ink to V\\S. Dead-ends are pages that have no link to any other web page. The problem of having them in our Page random algorithm is that, if we don’t have random jumps, the probability will depend on the starting point, which is something undesirable. \n",
    "    \n",
    "As we can see in our algorithm, we identify dead-ends, that means finding the nodes without-degree equals to zero. We have to do this recursively, as when we remove a node that was a dead end, we can create new dead ends (a node that just had a link in direction to the removed dead-end).\n",
    "    \n",
    "We created an example to see our algorithm working, as can be seen above.\n",
    "</blockquote>    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
